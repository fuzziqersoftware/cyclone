{
  // Cyclone configuration file.

  // Most fields can be updated automatically without restarting the server.
  // Cyclone will notice changes in this file within 10 seconds of the file
  // being written and apply the changes.
  // Fields that require restarts are explicitly noted below. Changing these
  // fields while the server is running is not harmful, but has no effect.

  // List of addresses on which to listen for HTTP connections. Each item in
  // this list must be one of the following formats:
  // - an int, to listen on that port on all interfaces.
  // - a string of "address:port", to listen on a specific interface.
  // - a string without a : in it, to listen on a Unix socket.
  // If no addresses are given, Cyclone will not run its HTTP server.
  // Changing this field requires restarting the server.
  "http_listen": [5050],

  // List of addresses on which to listen for connections using the Graphite
  // line stream protocol. This field's semantics are the same as for
  // http_listen above.
  // Changing this field requires restarting the server.
  "line_stream_listen": [2003],

  // List of addresses on which to listen for connections using the Graphite
  // line datagram protocol. This field's semantics are the same as for
  // http_listen above.
  // Changing this field requires restarting the server.
  "line_datagram_listen": [2003],

  // List of addresses on which to listen for connections using the Graphite
  // pickle protocol. This field's semantics are the same as for http_listen
  // above.
  // Changing this field requires restarting the server.
  "pickle_listen": [2004],

  // List of addresses on which to listen for connections using the Cyclone
  // shell protocol. This protocol is a way to interact with Cyclone from the
  // terminal for manual changes (writing datapoints manually, creating or
  // deleting series, etc.). This field's semantics are the same as for
  // http_listen above.
  // Changing this field requires restarting the server.
  "shell_listen": [2001],

  // Port on which to run the Thrift server. Unlike the other servers, only a
  // single port can be given here, and Unix sockets can't be used. If this is
  // set to zero, Cyclone will not run its Thrift server. The remote store (see
  // below) uses the Thrift interface, so this must not be zero if this node is
  // part of a cluster.
  // Changing this field requires restarting the server.
  "thrift_port": 2000,

  // Number of threads to service requests for each type of server. The line
  // stream, pickle, and shell interfaces are all handled by the stream server.
  // Changing any of these fields requires restarting the server.
  "http_threads": 4,
  "stream_threads": 4,
  "datagram_threads": 4,
  "thrift_threads": 4,

  // Cyclone generates metrics about internal operations every so often; this
  // specifies how often it writes them. To disable stats reporting, set this to
  // zero. Note that it generally doesn't make sense for this interval to be
  // shorter than the highest-resolution archive in the relevant database files
  // (see the autocreate rules section below).
  "stats_report_usecs": 60000000,

  // Maximum number of Whisper database files to keep open at once. If you
  // decrease this field by a large amount, the server may block queries for a
  // short time while evicting open files.
  "open_file_cache_size": 16384,

  // Number of cache directory levels to prepopulate. At startup, Cyclone will
  // run find queries starting at * for this many levels deep before starting
  // any servers. This may be useful in a high-write environment, where writes
  // can starve reads that need to populate the cache root (e.g. find(*)). If
  // omitted, the default value is 0 (no prepopulation occurs).
  "prepopulate_depth": 2,

  // Amount of information to write to stderr during operation. Larger numbers
  // mean less verbosity. The levels are as follows:
  // 0 - write everything, including debugging messages
  // 1 - write informational messages (e.g. autocreations), warnings, and errors
  // 2 - write warnings and errors only
  // 3 - write errors only
  // 4 - write nothing
  "log_level": 1,

  // Slow query threshold. Any query that takes longer than this amount of time
  // will be logged, including debugging information about various steps within
  // the query. If set to zero, all queries will be logged. If set to a negative
  // value, no queries will be logged. Note that queries are logged at the info
  // level (1), so if log_level is greater than 1, this field is meaningless.
  "profiler_threshold_usecs": -1,

  // Same as above, but this only applies to internal queries, such as those
  // done by the verify procedure or write buffer threads.
  "internal_profiler_threshold_usecs": -1,

  // The store configuration specifies how data is accessed. This configuration
  // consists of one or more types of stores organized in a hierarchy. There are
  // several types of stores, which can be configured as follows:
  //
  // DISK STORE
  // This store reads and writes data in whisper files in a local data
  // directory. It caches open file descriptors, but does not cache any file
  // metadata, so files can be modified on disk by other processes in the
  // background and Cyclone won't get confused. However, if you delete files,
  // they may still be open in Cyclone's cache, and won't get recreated if new
  // data is received that would be written to them. It's recommended to delete
  // entire series using the Thrift or shell interface instead.
  // {
  //   "store_type": "disk",
  //   "directory": "/path/to/cyclone/data",
  // }
  //
  // CACHED DISK STORE
  // Same as the disk store, but maintains a metadata cache on top of the disk,
  // reducing the number of reads on the underlying disk. Unlike the non-cached
  // disk store, operating on the underlying Whisper files by other processes
  // while the server is running may result in erroneous behavior. The directory
  // and file limits control the maximum number of items of each type that will
  // be kept in memory - when the item counts reach these limits, the cache will
  // evict the least recently used files and directories. Note that changing the
  // directory while the server is running will delete everything from the
  // cache, which may cause temporary performance issues.
  // {
  //   "store_type": "cached_disk",
  //   "directory": "/path/to/cyclone/data",
  //   "directory_limit": 100000,
  //   "file_limit": 10000000,
  // }
  //
  // HASH STORE
  // This store distributes data between multiple substores using a consistent
  // hashing algorithm. Each substore has a unique name which is used as its
  // hash key.
  // The precision parameter must be a nonzero integer. If the precision is
  // positive, a constant-time distribution algorithm is used, and the precision
  // specifies the number of bits used in the ring - the ring will use
  // 2^precision bytes of memory. If the precision is negative, the Carbon
  // consistent hashing algorithm is used, and (-precision) specifies the
  // replica count. If you want to run Cyclone alongside Carbon without
  // resharding, the store names must be of the form
  // "('hostname', 'graphite_instance_name')" to work around what appears to be
  // a bug in Carbon's consistent hashing logic. Changing the precision after
  // keys have been created will cause keys to exist in the wrong stores.
  // {
  //   "store_type": "hash",
  //   "precision": 20,
  //   "stores": {
  //     "store1": {...},
  //     "store2": {...},
  //     ...
  //   }
  // }
  //
  // MULTI STORE
  // This store replicates data between multiple substores. All queries are sent
  // to all stores, and the results are merged before returning to the caller.
  // {
  //   "store_type": "multi",
  //   "stores": {
  //     "store1": {...},
  //     "store2": {...},
  //     ...
  //   }
  // }
  //
  // REMOTE STORE
  // This store forwards all of its read and write requests to a remote Cyclone
  // server via the Thrift interface. This store uses persistent connections,
  // closing them only if the number of connections exceeds connection_limit or
  // when an error occurs. If connection_limit is 0, the connection cache size
  // is unlimited, and connections are only closed on errors. At any time, the
  // open connection count will be at least the number of currently-executing
  // remote queries, which can exceed the connection cache limit. This can be
  // limited by wrapping a remote store in a write buffer store; each thread of
  // the write buffer store makes only one query at a time. If the hostname or
  // port is changed, currently-executing queries will not be aborted; the store
  // will close connections to the old hostname and port when the query
  // completes.
  // {
  //   "store_type": "remote",
  //   "hostname": "remote-server1",
  //   "connection_limit": 8,
  //   "port": 2000, // thrift port on the remote server
  // }
  //
  // WRITE BUFFER STORE
  // This store wraps another store and batches writes to the underlying store.
  // The writes are flushed asynchronously in a set of background threads. Reads
  // are passed through this store to the underlying store synchronously, but
  // the results are merged with any pending writes waiting in the write buffer.
  // Writes can be rate-limited using the max_update_metadatas_per_second and
  // max_write_batches_per_second options. Rate limits only apply when the queue
  // is short; the disable_rate_limit_for_queue_length option controls this
  // threshold. Set it to a negative value to disable this behavior (and allow
  // the queue to grow arbitrarily). This should usually be a nonnegative value.
  // If merge_find_patterns is false, the results of find queries that include
  // patterns will not be merged with the write queue. This means that writes to
  // series that don't exist but will be autocreated won't be visible until the
  // write actually occurs. This can be useful if writes are heavily
  // rate-limited since the write queue could be long, which makes merging slow.
  // {
  //   "store_type": "write_buffer",
  //   "num_write_threads": 4,
  //   "batch_size": 1000, // number of series to write in a single write() call
  //   "max_update_metadatas_per_second": 0, // 0 means no rate limit
  //   "max_write_batches_per_second": 0, // 0 means no rate limit
  //   "disable_rate_limit_for_queue_length": 1000000,
  //   "merge_find_patterns": true,
  //   "substore": {...}, // config for the wrapped store
  // }
  //
  // QUERY STORE
  // This store wraps another store and provides function execution on top of
  // it. Writes and finds pass through this store unmodified; reads are parsed
  // as Graphite expressions and the functions are executed. This store is
  // required if you want to be able to call read() and pass in functions, but
  // it is NOT required if you just want to pass in patterns or individual
  // series names. When using the Graphite web frontend in front of Cyclone,
  // this store is not required - the frontend will only make pattern queries,
  // and will not pass through function calls to Cyclone. Note that very few
  // functions are currently implemented.
  // {
  //   "store_type": "query",
  //   "substore": {...}, // config for the wrapped store
  // }
  //
  // READ ONLY STORE
  // This store prevents writes to the underlying store.
  // {
  //   "store_type": "read_only",
  //   "substore": {...}, // config for the wrapped store
  // }
  //
  // EMPTY STORE
  // This store discards all writes and returns empty results for all reads,
  // sort of like MySQL's BLACKHOLE and Linux's /dev/null.
  // {
  //   "store_type": "empty",
  // }
  //
  // When using Cyclone in a clustered setup, usually a hash store should be
  // used to distribute work among the cluster. The hash store contains a
  // substore for each node in the cluster. The substore corresponding to the
  // local host should be some kind of disk store, likely wrapped in a write
  // buffer store, and the others should be remote stores. For example, in a
  // cluster consisting of node1, node2, node3, and node4, the store
  // configuration on node2 may look something like this:
  // {
  //   "type": "hash",
  //   "precision": -100,
  //   "stores": {
  //     "node1": {
  //       "type": "remote",
  //       "hostname": "node1",
  //       "port": 2000,
  //       "connection_limit": 32,
  //     },
  //     "node2": {
  //       "type": "write_buffer",
  //       "num_write_threads": 8,
  //       "batch_size": 100,
  //       "max_update_metadatas_per_second": 100,
  //       "max_write_batches_per_second": 5,
  //       "disable_rate_limit_for_queue_length": 1000000,
  //       "merge_find_results": false,
  //       "substore": {
  //         "type": "cached_disk",
  //         "directory": "/cyclone-data",
  //         "directory_limit": 15000000,
  //         "file_limit": 30000000,
  //       },
  //     },
  //     "node3": {
  //       "type": "remote",
  //       "hostname": "node3",
  //       "port": 2000,
  //       "connection_limit": 32,
  //     },
  //     "node4": {
  //       "type": "remote",
  //       "hostname": "node4",
  //       "port": 2000,
  //       "connection_limit": 32,
  //     },
  //   },
  // }
  //
  // Individual fields in store configurations can be changed without restarting
  // the server, but the structure of the store configuration cannot be changed.
  // For example, the type of a store cannot be changed, nor can stores be added
  // or removed - these operations all require a full restart.
  //
  // The default store configuration uses a cached disk store wrapped by a
  // write buffer store. This configuration is generally sufficient for a
  // standalone setup, though you may want to change the thread counts, rate
  // limits, and cache limits to match the configuration of the machine on which
  // it runs.
  "store_config": {
    "type": "write_buffer",
    "num_write_threads": 4,
    "batch_size": 100,
    "max_update_metadatas_per_second": 100,
    "max_write_batches_per_second": 100,
    "disable_rate_limit_for_queue_length": 1000000,
    "merge_find_patterns": false,
    "substore": {
      "type": "cached_disk",
      "directory_limit": 10000,
      "file_limit": 50000,
      "directory": "./cyclone-data",
    },
  },

  // Rules for automatic series creation. When Cyclone receives a write for
  // a series that doesn't exist, it checks if the key name matches any of these
  // rules (in order) and automatically creates the series with the given
  // parameters for the first rule that matches. If no rules match, the write is
  // discarded.
  //
  // Rules are 4-tuples of this format:
  // [key_pattern, retentions, x_files_factor, aggregation_method]
  //
  // -- The key_pattern is the pattern to match the keys against. In these
  //    patterns, [abc] matches the characters a, b, or c; {abc,def,ghi} matches
  //    the substrings "abc", "def", or "ghi"; * matches any number of
  //    characters except '.'; ** matches any number of characters including
  //    '.'.
  // -- Retentions are comma-separated pairs of "seconds_per_point:num_points".
  //    Either of these quantities may be expressed in a time length instead of
  //    a bare number; e.g. "1m:90d" means to retain one datapoint per minute
  //    for 90 days. This is equivalent to "60:129600" (there are 129600 minutes
  //    in 90 days).
  // -- The x_files_factor is the proportion of datapoints that must be present
  //    in each interval for a propagation to occur to the next lower
  //    resolution.
  // -- The aggregation_method is one of the following strings: "average",
  //    "sum", "last", "min", "max". This specifies how to combine datapoints
  //    into lower-resolution archives.
  //
  // The default rule autocreates any key that doesn't exist, with a minutely
  // retention for the past 90 days.
  "autocreate_rules": [
    ["**", "60:90d", 0, "average"],
  ],
}